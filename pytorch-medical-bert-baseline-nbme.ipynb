{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NBME - Score Clinical Patient Notes \n- **Framework:** Pytorch\n- **Model Architecture:**\n    - BERT\n    - Linear(768, 512)\n    - Linear(512, 512)\n    - Linear(512, 1)\n- **LR:** 1e-5\n- **Batch Size:** 8\n- **Epoch:** 3\n- **Dropout:** 0.2\n- **Criterion:** BCEWithLogitsLoss\n- **Optimizer:** AdamW\n\n# Tokenizer params\n- **Max Lenght:** 416\n- **Padding:** max_lenght\n- **Truncation:** only_scond\n","metadata":{}},{"cell_type":"code","source":"from ast import literal_eval\nfrom itertools import chain\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:17.686436Z","iopub.execute_input":"2022-03-17T18:12:17.687620Z","iopub.status.idle":"2022-03-17T18:12:17.696892Z","shell.execute_reply.started":"2022-03-17T18:12:17.687558Z","shell.execute_reply":"2022-03-17T18:12:17.695768Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions\n### 1. Datasets Helper Function\nneed to merge `features.csv`, `patient_notes.csv` with `train.csv`","metadata":{}},{"cell_type":"code","source":"BASE_URL = \"../input/nbme-score-clinical-patient-notes\"\n\n\ndef process_feature_text(text):\n    return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n\n\ndef prepare_datasets():\n    features = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    df = pd.read_csv(f\"{BASE_URL}/train.csv\")\n    df[\"annotation_list\"] = [literal_eval(x) for x in df[\"annotation\"]]\n    df[\"location_list\"] = [literal_eval(x) for x in df[\"location\"]]\n\n    merged = df.merge(notes, how=\"left\")\n    merged = merged.merge(features, how=\"left\")\n\n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n\n    return merged","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:17.700007Z","iopub.execute_input":"2022-03-17T18:12:17.700386Z","iopub.status.idle":"2022-03-17T18:12:17.712842Z","shell.execute_reply.started":"2022-03-17T18:12:17.700344Z","shell.execute_reply":"2022-03-17T18:12:17.711698Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"### 2. Tokenizer Helper Function","metadata":{}},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\n\ndef tokenize_and_add_labels(tokenizer, data, config):\n    out = tokenizer(\n        data[\"feature_text\"],\n        data[\"pn_history\"],\n        truncation=config['truncation'],\n        max_length=config['max_length'],\n        padding=config['padding'],\n        return_offsets_mapping=config['return_offsets_mapping']\n    )\n    labels = [0.0] * len(out[\"input_ids\"])\n    out[\"location_int\"] = loc_list_to_ints(data[\"location_list\"])\n    out[\"sequence_ids\"] = out.sequence_ids()\n\n    for idx, (seq_id, offsets) in enumerate(zip(out[\"sequence_ids\"], out[\"offset_mapping\"])):\n        if not seq_id or seq_id == 0:\n            labels[idx] = -1\n            continue\n\n        token_start, token_end = offsets\n        for feature_start, feature_end in out[\"location_int\"]:\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                break\n\n    out[\"labels\"] = labels\n\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:17.714671Z","iopub.execute_input":"2022-03-17T18:12:17.715140Z","iopub.status.idle":"2022-03-17T18:12:17.729148Z","shell.execute_reply.started":"2022-03-17T18:12:17.715098Z","shell.execute_reply":"2022-03-17T18:12:17.727275Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"### 3. Predection and Score Helper Function","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef get_location_predictions(preds, offset_mapping, sequence_ids, test=False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = 1 / (1 + np.exp(-pred))\n        start_idx = None\n        end_idx = None\n        current_preds = []\n        for pred, offset, seq_id in zip(pred, offsets, seq_ids):\n            if seq_id is None or seq_id == 0:\n                continue\n\n            if pred > 0.5:\n                if start_idx is None:\n                    start_idx = offset[0]\n                end_idx = offset[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n            \n    return all_predictions\n\n\ndef calculate_char_cv(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros(num_chars)\n\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n\n        char_preds = np.zeros(num_chars)\n\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n\n    results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", labels=np.unique(all_preds))\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n\n    return {\n        \"Accuracy\": accuracy,\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:17.731410Z","iopub.execute_input":"2022-03-17T18:12:17.732206Z","iopub.status.idle":"2022-03-17T18:12:17.749533Z","shell.execute_reply.started":"2022-03-17T18:12:17.732163Z","shell.execute_reply":"2022-03-17T18:12:17.748339Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data = self.data.iloc[idx]\n        tokens = tokenize_and_add_labels(self.tokenizer, data, self.config)\n\n        input_ids = np.array(tokens[\"input_ids\"])\n        attention_mask = np.array(tokens[\"attention_mask\"])\n        token_type_ids = np.array(tokens[\"token_type_ids\"])\n\n        labels = np.array(tokens[\"labels\"])\n        offset_mapping = np.array(tokens['offset_mapping'])\n        sequence_ids = np.array(tokens['sequence_ids']).astype(\"float16\")\n        \n        return input_ids, attention_mask, token_type_ids, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:17.752833Z","iopub.execute_input":"2022-03-17T18:12:17.753959Z","iopub.status.idle":"2022-03-17T18:12:17.765202Z","shell.execute_reply.started":"2022-03-17T18:12:17.753899Z","shell.execute_reply":"2022-03-17T18:12:17.763844Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- Lets use **BERT** base Architecture\n- Also Used 3 FC layers\n\n**Comments:** 3 layers improve accuracy 2% on public score","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass CustomModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(config['model_name'])  # BERT model\n        self.dropout = nn.Dropout(p=config['dropout'])\n        self.config = config\n        self.fc1 = nn.Linear(768, 512)\n        #self.fc2 = nn.Linear(512, 512)\n        self.fc2 = nn.Linear(512, 1)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        logits = F.relu(self.fc1(outputs[0]))\n        #logits = self.fc2(self.dropout(logits))\n        logits = self.fc2(self.dropout(logits)).squeeze(-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:17.766680Z","iopub.execute_input":"2022-03-17T18:12:17.767737Z","iopub.status.idle":"2022-03-17T18:12:17.780676Z","shell.execute_reply.started":"2022-03-17T18:12:17.767695Z","shell.execute_reply":"2022-03-17T18:12:17.779579Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters\n","metadata":{}},{"cell_type":"code","source":"hyperparameters = {\n    \"max_length\": 416,\n    \"padding\": \"max_length\",\n    \"return_offsets_mapping\": True,\n    \"truncation\": \"only_second\",\n    \"model_name\": \"../input/layoutlm/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n    \"dropout\": 0.2,\n    \"lr\": 1e-5,\n    \"test_size\": 0.2,\n    \"seed\": 1268,\n    \"batch_size\": 8\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:17.782364Z","iopub.execute_input":"2022-03-17T18:12:17.783432Z","iopub.status.idle":"2022-03-17T18:12:17.792030Z","shell.execute_reply.started":"2022-03-17T18:12:17.783390Z","shell.execute_reply":"2022-03-17T18:12:17.791142Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Datasets\nTrain and Test split: 20%\n\nTotal Data:\n- Train: 11440\n- Test: 2860","metadata":{}},{"cell_type":"code","source":"train_df = prepare_datasets()\n\nX_train, X_test = train_test_split(train_df, test_size=hyperparameters['test_size'],\n                                   random_state=hyperparameters['seed'])\n\n\nprint(\"Train size\", len(X_train))\nprint(\"Test Size\", len(X_test))","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:17.793769Z","iopub.execute_input":"2022-03-17T18:12:17.794906Z","iopub.status.idle":"2022-03-17T18:12:18.588685Z","shell.execute_reply.started":"2022-03-17T18:12:17.794833Z","shell.execute_reply":"2022-03-17T18:12:18.587543Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Train size 11440\nTest Size 2860\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(hyperparameters['model_name'])\n\ntraining_data = CustomDataset(X_train, tokenizer, hyperparameters)\ntrain_dataloader = DataLoader(training_data, batch_size=hyperparameters['batch_size'], shuffle=True)\n\ntest_data = CustomDataset(X_test, tokenizer, hyperparameters)\ntest_dataloader = DataLoader(test_data, batch_size=hyperparameters['batch_size'], shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:18.590213Z","iopub.execute_input":"2022-03-17T18:12:18.590691Z","iopub.status.idle":"2022-03-17T18:12:18.672940Z","shell.execute_reply.started":"2022-03-17T18:12:18.590646Z","shell.execute_reply":"2022-03-17T18:12:18.671910Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"# Train\nLets train the model\nwith BCEWithLogitsLoss and AdamW as optimizer\n\n**Notes:** on BCEWithLogitsLoss, the default value for reduction is `mean` (the sum of the output will be divided by the number of elements in the output). If we use this default value, it will produce negative loss. Because we have some negative labels. To fix this negative loss issue, we can use `none` as parameter. To calculate the mean, first, we have to filter out the negative values. [DOC](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = CustomModel(hyperparameters).to(DEVICE)\n\ncriterion = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\noptimizer = optim.AdamW(model.parameters(), lr=hyperparameters['lr'])","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:18.674850Z","iopub.execute_input":"2022-03-17T18:12:18.675510Z","iopub.status.idle":"2022-03-17T18:12:26.294965Z","shell.execute_reply.started":"2022-03-17T18:12:18.675462Z","shell.execute_reply":"2022-03-17T18:12:26.293884Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, criterion):\n        model.train()\n        train_loss = []\n\n        for batch in tqdm(dataloader):\n            optimizer.zero_grad()\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            token_type_ids = batch[2].to(DEVICE)\n            labels = batch[3].to(DEVICE)\n\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = criterion(logits, labels)\n            # since, we have\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            train_loss.append(loss.item() * input_ids.size(0))\n            loss.backward()\n            # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n            # it's also improve f1 accuracy slightly\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n        return sum(train_loss)/len(train_loss)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:26.297017Z","iopub.execute_input":"2022-03-17T18:12:26.297637Z","iopub.status.idle":"2022-03-17T18:12:26.310595Z","shell.execute_reply.started":"2022-03-17T18:12:26.297590Z","shell.execute_reply":"2022-03-17T18:12:26.309408Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, dataloader, criterion):\n        model.eval()\n        valid_loss = []\n        preds = []\n        offsets = []\n        seq_ids = []\n        valid_labels = []\n\n        for batch in tqdm(dataloader):\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            token_type_ids = batch[2].to(DEVICE)\n            labels = batch[3].to(DEVICE)\n            offset_mapping = batch[4]\n            sequence_ids = batch[5]\n\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = criterion(logits, labels)\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            valid_loss.append(loss.item() * input_ids.size(0))\n\n            preds.append(logits.detach().cpu().numpy())\n            offsets.append(offset_mapping.numpy())\n            seq_ids.append(sequence_ids.numpy())\n            valid_labels.append(labels.detach().cpu().numpy())\n\n        preds = np.concatenate(preds, axis=0)\n        offsets = np.concatenate(offsets, axis=0)\n        seq_ids = np.concatenate(seq_ids, axis=0)\n        valid_labels = np.concatenate(valid_labels, axis=0)\n        location_preds = get_location_predictions(preds, offsets, seq_ids, test=False)\n        score = calculate_char_cv(location_preds, offsets, seq_ids, valid_labels)\n\n        return sum(valid_loss)/len(valid_loss), score","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:26.312407Z","iopub.execute_input":"2022-03-17T18:12:26.313083Z","iopub.status.idle":"2022-03-17T18:12:26.331041Z","shell.execute_reply.started":"2022-03-17T18:12:26.313034Z","shell.execute_reply":"2022-03-17T18:12:26.329901Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"import time\n\ntrain_loss_data, valid_loss_data = [], []\nscore_data_list = []\nvalid_loss_min = np.Inf\nsince = time.time()\nepochs = 10","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:26.332904Z","iopub.execute_input":"2022-03-17T18:12:26.333535Z","iopub.status.idle":"2022-03-17T18:12:26.345984Z","shell.execute_reply.started":"2022-03-17T18:12:26.333488Z","shell.execute_reply":"2022-03-17T18:12:26.344728Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"best_loss = np.inf\n\nfor i in range(epochs):\n    print(\"Epoch: {}/{}\".format(i + 1, epochs))\n    # first train model\n    train_loss = train_model(model, train_dataloader, optimizer, criterion)\n    train_loss_data.append(train_loss)\n    print(f\"Train loss: {train_loss}\")\n    # evaluate model\n    valid_loss, score = eval_model(model, test_dataloader, criterion)\n    valid_loss_data.append(valid_loss)\n    score_data_list.append(score)\n    print(f\"Valid loss: {valid_loss}\")\n    print(f\"Valid score: {score}\")\n    \n    if valid_loss < best_loss:\n        best_loss = valid_loss\n        torch.save(model.state_dict(), \"nbme_bert_v2.pth\")\n\n    \ntime_elapsed = time.time() - since\nprint('Training completed in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))","metadata":{"execution":{"iopub.status.busy":"2022-03-17T18:12:30.756953Z","iopub.execute_input":"2022-03-17T18:12:30.757285Z","iopub.status.idle":"2022-03-17T20:02:34.121571Z","shell.execute_reply.started":"2022-03-17T18:12:30.757241Z","shell.execute_reply":"2022-03-17T20:02:34.120498Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Epoch: 1/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"133d2d40b70a47a4a78b4296e509f417"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.3435550393341569\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f81bd268727a4e51a7d1fd2d413a5a4b"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.12723477077678685\nValid score: {'Accuracy': 0.9933333561340477, 'precision': 0.7780053845253424, 'recall': 0.7685147713476326, 'f1': 0.7732309571590612}\nEpoch: 2/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4437764fec34ad7afa279a9ae4835a5"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.11817858422437928\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f32d606f52c248febf750332847b7152"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.11001517459607574\nValid score: {'Accuracy': 0.9937587319610723, 'precision': 0.7562344619012226, 'recall': 0.8529224721049893, 'f1': 0.8016736629036422}\nEpoch: 3/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ca2e524c316458c93f624b2a63421ea"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.0876867426143124\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b949f7130624c5282458d1921032407"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.11841252530343269\nValid score: {'Accuracy': 0.9930969412271686, 'precision': 0.7103276856771487, 'recall': 0.9004451639012545, 'f1': 0.7941667622726172}\nEpoch: 4/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d966d315d87c4b7eb10781ab289c761f"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.06719492150852167\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67a6c4c258cf4792ad3ef6df5607920c"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.11683786569176932\nValid score: {'Accuracy': 0.9939528230419459, 'precision': 0.7650413458796692, 'recall': 0.8531248193328322, 'f1': 0.8066857088190675}\nEpoch: 5/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734a2e8e653d40c8ac7a2b89d881fbfa"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.05393378603680868\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd0d92b40bee4c89a19e1b9feee40918"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.12261236112163605\nValid score: {'Accuracy': 0.9941387913683336, 'precision': 0.7761703253107644, 'recall': 0.8483262993582702, 'f1': 0.8106458206728911}\nEpoch: 6/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e086d294d97a404f8bea12dfd59f5a40"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.04469160333127867\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d610d82a6cc447e9739f5c482707d95"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.11403765072004138\nValid score: {'Accuracy': 0.9943063766187795, 'precision': 0.7775226964416153, 'recall': 0.8615366826617332, 'f1': 0.817376518662754}\nEpoch: 7/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd478f66a5af438ab05529adfd7f8442"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.035946246349783176\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"272e9758c7384fa690d45e4fa01014ea"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.1214658945406331\nValid score: {'Accuracy': 0.9942247215605264, 'precision': 0.7734121735522187, 'recall': 0.862057004104758, 'f1': 0.8153322488483042}\nEpoch: 8/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ac9eebf35204bceae1e701623b5aea4"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.03033832082452779\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18bf93c886fc459c8afe2fe9a9a69a2b"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.12999363164563726\nValid score: {'Accuracy': 0.994428645449462, 'precision': 0.7900457358084477, 'recall': 0.8488755275481298, 'f1': 0.8184047711944709}\nEpoch: 9/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"281cc4ba954e4e82abd8a11f5e3deb7c"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.02610563010883681\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccfc5e4d3c9f46eda0c17aee4db06517"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.14135069977865267\nValid score: {'Accuracy': 0.9945218433693528, 'precision': 0.7959721693754417, 'recall': 0.8465918945481875, 'f1': 0.8205020451616519}\nEpoch: 10/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9cc66d90f584ea292bb79e9d86c092b"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.021460353362805756\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"864af8b142b7495785ddd216c94be84a"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.14418845260126217\nValid score: {'Accuracy': 0.994264480306168, 'precision': 0.7608256564362776, 'recall': 0.8928715962305602, 'f1': 0.8215767634854771}\nTraining completed in 110m 8s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Experimets:\n- exp 1\nParams: Base bert with 1FC, epoch 5, lr 1e-5\n\n{'Accuracy': 0.9922235313632376, 'precision': 0.699022058288238, 'recall': 0.8327118203104674, 'f1': 0.7600327168148598}\n\n- exp 2:\nParams: Base bert with 2FC, epoch 2, lr 1e-5\n\n{'Accuracy': 0.9931995444417273, 'precision': 0.755762387079113, 'recall': 0.7980805365247304, 'f1': 0.7763452047860748}\n\n- exp 3:\nparams: 2FC, epoch 2, lr 1e-5 with gradient clip\n{'Accuracy': 0.9932764968526464, 'precision': 0.7633003963601853, 'recall': 0.7905067499205042, 'f1': 0.7766653886025079}\n\n- exp 4: 3FC, epoch 2, 1e-5 with gradient clip\n\n{'Accuracy': 0.9933637095850213, 'precision': 0.7576469952442715, 'recall': 0.8105397045645073, 'f1': 0.7832013519364255}\n","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.plot(train_loss_data, label=\"Training loss\")\nplt.plot(valid_loss_data, label=\"validation loss\")\nplt.legend(frameon=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:51:19.579429Z","iopub.execute_input":"2022-02-14T17:51:19.579819Z","iopub.status.idle":"2022-02-14T17:51:19.830516Z","shell.execute_reply.started":"2022-02-14T17:51:19.579786Z","shell.execute_reply":"2022-02-14T17:51:19.829766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nscore_df = pd.DataFrame.from_dict(score_data_list)\nscore_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:51:22.937258Z","iopub.execute_input":"2022-02-14T17:51:22.937911Z","iopub.status.idle":"2022-02-14T17:51:22.949599Z","shell.execute_reply.started":"2022-02-14T17:51:22.937875Z","shell.execute_reply":"2022-02-14T17:51:22.948962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare For Testing","metadata":{}},{"cell_type":"markdown","source":"Load best model","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"nbme_bert_v2.pth\", map_location = DEVICE))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:51:33.339694Z","iopub.execute_input":"2022-02-14T17:51:33.339963Z","iopub.status.idle":"2022-02-14T17:51:33.565648Z","shell.execute_reply.started":"2022-02-14T17:51:33.339918Z","shell.execute_reply":"2022-02-14T17:51:33.564786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_df():\n    feats = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    test = pd.read_csv(f\"{BASE_URL}/test.csv\")\n\n    merged = test.merge(notes, how = \"left\")\n    merged = merged.merge(feats, how = \"left\")\n\n    def process_feature_text(text):\n        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n    \n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    \n    return merged\n\n\nclass SubmissionDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = self.tokenizer(\n            example[\"feature_text\"],\n            example[\"pn_history\"],\n            truncation = self.config['truncation'],\n            max_length = self.config['max_length'],\n            padding = self.config['padding'],\n            return_offsets_mapping = self.config['return_offsets_mapping']\n        )\n        tokenized[\"sequence_ids\"] = tokenized.sequence_ids()\n\n        input_ids = np.array(tokenized[\"input_ids\"])\n        attention_mask = np.array(tokenized[\"attention_mask\"])\n        token_type_ids = np.array(tokenized[\"token_type_ids\"])\n        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\")\n\n        return input_ids, attention_mask, token_type_ids, offset_mapping, sequence_ids\n\n\ntest_df = create_test_df()\n\nsubmission_data = SubmissionDataset(test_df, tokenizer, hyperparameters)\nsubmission_dataloader = DataLoader(submission_data, batch_size=hyperparameters['batch_size'], shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:14.28865Z","iopub.execute_input":"2022-02-15T04:43:14.288938Z","iopub.status.idle":"2022-02-15T04:43:14.615585Z","shell.execute_reply.started":"2022-02-15T04:43:14.288889Z","shell.execute_reply":"2022-02-15T04:43:14.614876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\npreds = []\noffsets = []\nseq_ids = []\n\nfor batch in tqdm(submission_dataloader):\n    input_ids = batch[0].to(DEVICE)\n    attention_mask = batch[1].to(DEVICE)\n    token_type_ids = batch[2].to(DEVICE)\n    offset_mapping = batch[3]\n    sequence_ids = batch[4]\n\n    logits = model(input_ids, attention_mask, token_type_ids)\n    \n    preds.append(logits.detach().cpu().numpy())\n    offsets.append(offset_mapping.numpy())\n    seq_ids.append(sequence_ids.numpy())\n\npreds = np.concatenate(preds, axis=0)\noffsets = np.concatenate(offsets, axis=0)\nseq_ids = np.concatenate(seq_ids, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:18.251294Z","iopub.execute_input":"2022-02-15T04:43:18.25185Z","iopub.status.idle":"2022-02-15T04:43:18.391331Z","shell.execute_reply.started":"2022-02-15T04:43:18.251811Z","shell.execute_reply":"2022-02-15T04:43:18.390536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_preds = get_location_predictions(preds, offsets, seq_ids, test=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:22.467394Z","iopub.execute_input":"2022-02-15T04:43:22.467991Z","iopub.status.idle":"2022-02-15T04:43:22.486548Z","shell.execute_reply.started":"2022-02-15T04:43:22.467953Z","shell.execute_reply":"2022-02-15T04:43:22.485852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(location_preds), len(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:24.683747Z","iopub.execute_input":"2022-02-15T04:43:24.684313Z","iopub.status.idle":"2022-02-15T04:43:24.690863Z","shell.execute_reply.started":"2022-02-15T04:43:24.684272Z","shell.execute_reply":"2022-02-15T04:43:24.689946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"location\"] = location_preds","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:26.979602Z","iopub.execute_input":"2022-02-15T04:43:26.980157Z","iopub.status.idle":"2022-02-15T04:43:26.98502Z","shell.execute_reply.started":"2022-02-15T04:43:26.980118Z","shell.execute_reply":"2022-02-15T04:43:26.984202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[[\"id\", \"location\"]].to_csv(\"submission.csv\", index = False)\npd.read_csv(\"submission.csv\").head()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:29.07465Z","iopub.execute_input":"2022-02-15T04:43:29.074938Z","iopub.status.idle":"2022-02-15T04:43:29.093071Z","shell.execute_reply.started":"2022-02-15T04:43:29.07489Z","shell.execute_reply":"2022-02-15T04:43:29.092348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Special Credits\n- [tomohiroh](https://www.kaggle.com/tomohiroh/nbme-bert-for-beginners)\n- [gazu468](https://www.kaggle.com/gazu468/nbme-details-eda)\n","metadata":{}}]}