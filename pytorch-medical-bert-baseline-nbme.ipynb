{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NBME - Score Clinical Patient Notes \n- **Framework:** Pytorch\n- **Model Architecture:**\n    - BERT\n    - Linear(768, 512)\n    - Linear(512, 512)\n    - Linear(512, 1)\n- **LR:** 1e-5\n- **Batch Size:** 8\n- **Epoch:** 3\n- **Dropout:** 0.2\n- **Criterion:** BCEWithLogitsLoss\n- **Optimizer:** AdamW\n\n# Tokenizer params\n- **Max Lenght:** 416\n- **Padding:** max_lenght\n- **Truncation:** only_scond\n","metadata":{}},{"cell_type":"code","source":"from ast import literal_eval\nfrom itertools import chain\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:02:21.028916Z","iopub.execute_input":"2022-03-17T16:02:21.029299Z","iopub.status.idle":"2022-03-17T16:02:21.038125Z","shell.execute_reply.started":"2022-03-17T16:02:21.029248Z","shell.execute_reply":"2022-03-17T16:02:21.036799Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions\n### 1. Datasets Helper Function\nneed to merge `features.csv`, `patient_notes.csv` with `train.csv`","metadata":{}},{"cell_type":"code","source":"BASE_URL = \"../input/nbme-score-clinical-patient-notes\"\n\n\ndef process_feature_text(text):\n    return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n\n\ndef prepare_datasets():\n    features = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    df = pd.read_csv(f\"{BASE_URL}/train.csv\")\n    df[\"annotation_list\"] = [literal_eval(x) for x in df[\"annotation\"]]\n    df[\"location_list\"] = [literal_eval(x) for x in df[\"location\"]]\n\n    merged = df.merge(notes, how=\"left\")\n    merged = merged.merge(features, how=\"left\")\n\n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n\n    return merged","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:02:21.040837Z","iopub.execute_input":"2022-03-17T16:02:21.041433Z","iopub.status.idle":"2022-03-17T16:02:21.055067Z","shell.execute_reply.started":"2022-03-17T16:02:21.041356Z","shell.execute_reply":"2022-03-17T16:02:21.053733Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### 2. Tokenizer Helper Function","metadata":{}},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\n\ndef tokenize_and_add_labels(tokenizer, data, config):\n    out = tokenizer(\n        data[\"feature_text\"],\n        data[\"pn_history\"],\n        truncation=config['truncation'],\n        max_length=config['max_length'],\n        padding=config['padding'],\n        return_offsets_mapping=config['return_offsets_mapping']\n    )\n    labels = [0.0] * len(out[\"input_ids\"])\n    out[\"location_int\"] = loc_list_to_ints(data[\"location_list\"])\n    out[\"sequence_ids\"] = out.sequence_ids()\n\n    for idx, (seq_id, offsets) in enumerate(zip(out[\"sequence_ids\"], out[\"offset_mapping\"])):\n        if not seq_id or seq_id == 0:\n            labels[idx] = -1\n            continue\n\n        token_start, token_end = offsets\n        for feature_start, feature_end in out[\"location_int\"]:\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                break\n\n    out[\"labels\"] = labels\n\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:02:21.057091Z","iopub.execute_input":"2022-03-17T16:02:21.057532Z","iopub.status.idle":"2022-03-17T16:02:21.071297Z","shell.execute_reply.started":"2022-03-17T16:02:21.057489Z","shell.execute_reply":"2022-03-17T16:02:21.070020Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"### 3. Predection and Score Helper Function","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef get_location_predictions(preds, offset_mapping, sequence_ids, test=False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = 1 / (1 + np.exp(-pred))\n        start_idx = None\n        end_idx = None\n        current_preds = []\n        for pred, offset, seq_id in zip(pred, offsets, seq_ids):\n            if seq_id is None or seq_id == 0:\n                continue\n\n            if pred > 0.5:\n                if start_idx is None:\n                    start_idx = offset[0]\n                end_idx = offset[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n            \n    return all_predictions\n\n\ndef calculate_char_cv(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros(num_chars)\n\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n\n        char_preds = np.zeros(num_chars)\n\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n\n    results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", labels=np.unique(all_preds))\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n\n    return {\n        \"Accuracy\": accuracy,\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:02:21.073669Z","iopub.execute_input":"2022-03-17T16:02:21.074445Z","iopub.status.idle":"2022-03-17T16:02:21.093947Z","shell.execute_reply.started":"2022-03-17T16:02:21.074395Z","shell.execute_reply":"2022-03-17T16:02:21.092412Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data = self.data.iloc[idx]\n        tokens = tokenize_and_add_labels(self.tokenizer, data, self.config)\n\n        input_ids = np.array(tokens[\"input_ids\"])\n        attention_mask = np.array(tokens[\"attention_mask\"])\n        token_type_ids = np.array(tokens[\"token_type_ids\"])\n\n        labels = np.array(tokens[\"labels\"])\n        offset_mapping = np.array(tokens['offset_mapping'])\n        sequence_ids = np.array(tokens['sequence_ids']).astype(\"float16\")\n        \n        return input_ids, attention_mask, token_type_ids, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:02:21.097802Z","iopub.execute_input":"2022-03-17T16:02:21.098403Z","iopub.status.idle":"2022-03-17T16:02:21.110550Z","shell.execute_reply.started":"2022-03-17T16:02:21.098345Z","shell.execute_reply":"2022-03-17T16:02:21.109432Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- Lets use **BERT** base Architecture\n- Also Used 3 FC layers\n\n**Comments:** 3 layers improve accuracy 2% on public score","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass CustomModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(config['model_name'])  # BERT model\n        self.dropout = nn.Dropout(p=config['dropout'])\n        self.config = config\n        self.fc1 = nn.Linear(768, 512)\n        #self.fc2 = nn.Linear(512, 512)\n        self.fc2 = nn.Linear(512, 1)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        logits = F.relu(self.fc1(outputs[0]))\n        #logits = self.fc2(self.dropout(logits))\n        logits = self.fc2(self.dropout(logits)).squeeze(-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:02:21.112247Z","iopub.execute_input":"2022-03-17T16:02:21.112771Z","iopub.status.idle":"2022-03-17T16:02:21.126246Z","shell.execute_reply.started":"2022-03-17T16:02:21.112722Z","shell.execute_reply":"2022-03-17T16:02:21.125158Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters\n","metadata":{}},{"cell_type":"code","source":"hyperparameters = {\n    \"max_length\": 416,\n    \"padding\": \"max_length\",\n    \"return_offsets_mapping\": True,\n    \"truncation\": \"only_second\",\n    \"model_name\": \"../input/layoutlm/Bio_ClinicalBERT\",\n    \"dropout\": 0.2,\n    \"lr\": 1e-5,\n    \"test_size\": 0.2,\n    \"seed\": 1268,\n    \"batch_size\": 8\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:02:21.128293Z","iopub.execute_input":"2022-03-17T16:02:21.128946Z","iopub.status.idle":"2022-03-17T16:02:21.137583Z","shell.execute_reply.started":"2022-03-17T16:02:21.128857Z","shell.execute_reply":"2022-03-17T16:02:21.136577Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Datasets\nTrain and Test split: 20%\n\nTotal Data:\n- Train: 11440\n- Test: 2860","metadata":{}},{"cell_type":"code","source":"train_df = prepare_datasets()\n\nX_train, X_test = train_test_split(train_df, test_size=hyperparameters['test_size'],\n                                   random_state=hyperparameters['seed'])\n\n\nprint(\"Train size\", len(X_train))\nprint(\"Test Size\", len(X_test))","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:02:21.139084Z","iopub.execute_input":"2022-03-17T16:02:21.141302Z","iopub.status.idle":"2022-03-17T16:02:21.821638Z","shell.execute_reply.started":"2022-03-17T16:02:21.141265Z","shell.execute_reply":"2022-03-17T16:02:21.820581Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Train size 11440\nTest Size 2860\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(hyperparameters['model_name'])\n\ntraining_data = CustomDataset(X_train, tokenizer, hyperparameters)\ntrain_dataloader = DataLoader(training_data, batch_size=hyperparameters['batch_size'], shuffle=True)\n\ntest_data = CustomDataset(X_test, tokenizer, hyperparameters)\ntest_dataloader = DataLoader(test_data, batch_size=hyperparameters['batch_size'], shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:02:21.823265Z","iopub.execute_input":"2022-03-17T16:02:21.823656Z","iopub.status.idle":"2022-03-17T16:02:21.862684Z","shell.execute_reply.started":"2022-03-17T16:02:21.823610Z","shell.execute_reply":"2022-03-17T16:02:21.861549Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# Train\nLets train the model\nwith BCEWithLogitsLoss and AdamW as optimizer\n\n**Notes:** on BCEWithLogitsLoss, the default value for reduction is `mean` (the sum of the output will be divided by the number of elements in the output). If we use this default value, it will produce negative loss. Because we have some negative labels. To fix this negative loss issue, we can use `none` as parameter. To calculate the mean, first, we have to filter out the negative values. [DOC](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = CustomModel(hyperparameters).to(DEVICE)\n\ncriterion = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\noptimizer = optim.AdamW(model.parameters(), lr=hyperparameters['lr'])","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:02:21.864294Z","iopub.execute_input":"2022-03-17T16:02:21.864617Z","iopub.status.idle":"2022-03-17T16:02:23.381481Z","shell.execute_reply.started":"2022-03-17T16:02:21.864574Z","shell.execute_reply":"2022-03-17T16:02:23.380370Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, criterion):\n        model.train()\n        train_loss = []\n\n        for batch in tqdm(dataloader):\n            optimizer.zero_grad()\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            token_type_ids = batch[2].to(DEVICE)\n            labels = batch[3].to(DEVICE)\n\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = criterion(logits, labels)\n            # since, we have\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            train_loss.append(loss.item() * input_ids.size(0))\n            loss.backward()\n            # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n            # it's also improve f1 accuracy slightly\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n        return sum(train_loss)/len(train_loss)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:02:23.383432Z","iopub.execute_input":"2022-03-17T16:02:23.383746Z","iopub.status.idle":"2022-03-17T16:02:23.394077Z","shell.execute_reply.started":"2022-03-17T16:02:23.383700Z","shell.execute_reply":"2022-03-17T16:02:23.392807Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, dataloader, criterion):\n        model.eval()\n        valid_loss = []\n        preds = []\n        offsets = []\n        seq_ids = []\n        valid_labels = []\n\n        for batch in tqdm(dataloader):\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            token_type_ids = batch[2].to(DEVICE)\n            labels = batch[3].to(DEVICE)\n            offset_mapping = batch[4]\n            sequence_ids = batch[5]\n\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = criterion(logits, labels)\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            valid_loss.append(loss.item() * input_ids.size(0))\n\n            preds.append(logits.detach().cpu().numpy())\n            offsets.append(offset_mapping.numpy())\n            seq_ids.append(sequence_ids.numpy())\n            valid_labels.append(labels.detach().cpu().numpy())\n\n        preds = np.concatenate(preds, axis=0)\n        offsets = np.concatenate(offsets, axis=0)\n        seq_ids = np.concatenate(seq_ids, axis=0)\n        valid_labels = np.concatenate(valid_labels, axis=0)\n        location_preds = get_location_predictions(preds, offsets, seq_ids, test=False)\n        score = calculate_char_cv(location_preds, offsets, seq_ids, valid_labels)\n\n        return sum(valid_loss)/len(valid_loss), score","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:02:23.396413Z","iopub.execute_input":"2022-03-17T16:02:23.397141Z","iopub.status.idle":"2022-03-17T16:02:23.411839Z","shell.execute_reply.started":"2022-03-17T16:02:23.397079Z","shell.execute_reply":"2022-03-17T16:02:23.410807Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"import time\n\ntrain_loss_data, valid_loss_data = [], []\nscore_data_list = []\nvalid_loss_min = np.Inf\nsince = time.time()\nepochs = 10","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:04:19.283082Z","iopub.execute_input":"2022-03-17T16:04:19.283398Z","iopub.status.idle":"2022-03-17T16:04:19.289950Z","shell.execute_reply.started":"2022-03-17T16:04:19.283365Z","shell.execute_reply":"2022-03-17T16:04:19.288901Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"best_loss = np.inf\n\nfor i in range(epochs):\n    print(\"Epoch: {}/{}\".format(i + 1, epochs))\n    # first train model\n    train_loss = train_model(model, train_dataloader, optimizer, criterion)\n    train_loss_data.append(train_loss)\n    print(f\"Train loss: {train_loss}\")\n    # evaluate model\n    valid_loss, score = eval_model(model, test_dataloader, criterion)\n    valid_loss_data.append(valid_loss)\n    score_data_list.append(score)\n    print(f\"Valid loss: {valid_loss}\")\n    print(f\"Valid score: {score}\")\n    \n    if valid_loss < best_loss:\n        best_loss = valid_loss\n        torch.save(model.state_dict(), \"nbme_bert_v2.pth\")\n\n    \ntime_elapsed = time.time() - since\nprint('Training completed in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:04:24.670852Z","iopub.execute_input":"2022-03-17T16:04:24.671518Z","iopub.status.idle":"2022-03-17T17:55:22.337210Z","shell.execute_reply.started":"2022-03-17T16:04:24.671480Z","shell.execute_reply":"2022-03-17T17:55:22.335077Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Epoch: 1/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d438d1fc61dc4ba4b14b1a587d1b3693"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.3183901668802111\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"569ba2aa6b5b4acaafa75cb80d2faf79"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.15292611940472445\nValid score: {'Accuracy': 0.9919208518802894, 'precision': 0.7038258746526064, 'recall': 0.7833376694707022, 'f1': 0.7414562070758202}\nEpoch: 2/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c09fe521a5f44a57b5d9c165d29ed3d8"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.1264466002411352\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a03159fa83a441ed897dfe2b86531bc1"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.11977852654324506\nValid score: {'Accuracy': 0.9931371274862042, 'precision': 0.7430009436929852, 'recall': 0.8193565172144653, 'f1': 0.77931290468924}\nEpoch: 3/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82e2c98f70da4b94967d5a383054462c"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.08951457724625558\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c87ff4a21a1d49cdb4839356e535bef8"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.11319706777681658\nValid score: {'Accuracy': 0.9933406238617456, 'precision': 0.7387505022097228, 'recall': 0.8504610759402191, 'f1': 0.790679549027776}\nEpoch: 4/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fa2d8ed66184e9ab7dcb9e75983e5ea"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.06971704359559427\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"127f3b36139a4b01850059d8955be47d"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.10938524707667183\nValid score: {'Accuracy': 0.9935077815987975, 'precision': 0.7432747511720611, 'recall': 0.8570230971583847, 'f1': 0.7961063372717508}\nEpoch: 5/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c615420712b49fcbe6300595605000c"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.055040722935898165\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43ea2371a36a4630bad19a58a9ea06dc"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.12245080041841325\nValid score: {'Accuracy': 0.9935958493579604, 'precision': 0.7453035495409861, 'recall': 0.8613014193622988, 'f1': 0.7991149255732869}\nEpoch: 6/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aa788f9a81c4806b5866a2813f2155f"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.044138440925106916\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04bd0f7ce9d1405a9bd9d14575145d21"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.12571797719475497\nValid score: {'Accuracy': 0.9938745880908448, 'precision': 0.7731647480656728, 'recall': 0.8290405573381898, 'f1': 0.8001283374717517}\nEpoch: 7/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"690d20c26ada466b9514cd4f6780d9bd"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.035768903229662787\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90ac9c2b86ed4692a0fb1dd5f96bf510"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.1494882931224374\nValid score: {'Accuracy': 0.9939468378544299, 'precision': 0.780902892334763, 'recall': 0.8210620645795392, 'f1': 0.8004791094201367}\nEpoch: 8/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecc910a6cd9d44ce910f547a35d9aaa2"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.02974238609359843\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"900341723c4a4c93a3c88de21db463e8"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.12695538809799178\nValid score: {'Accuracy': 0.9937647171485883, 'precision': 0.7553343542623787, 'recall': 0.855490995288064, 'f1': 0.8022989440581242}\nEpoch: 9/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c880dd7d3002477cb0b9a51513edc1ca"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.024415459460022285\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a12700fe40d4e1c9ee4fa08346c9aaa"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.15711379962026967\nValid score: {'Accuracy': 0.9939972844349213, 'precision': 0.7752893270467209, 'recall': 0.8365854363599572, 'f1': 0.8047719025041364}\nEpoch: 10/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"795f06d4690a44c98f6fbc1fa44acb7c"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.020115680240803974\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc5eb5ce213a45d8b7232e3f93cafb75"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.16210081403706594\nValid score: {'Accuracy': 0.9936112398401442, 'precision': 0.7466050854690128, 'recall': 0.8598271326568959, 'f1': 0.7992261392949269}\nTraining completed in 111m 3s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Experimets:\n- exp 1\nParams: Base bert with 1FC, epoch 5, lr 1e-5\n\n{'Accuracy': 0.9922235313632376, 'precision': 0.699022058288238, 'recall': 0.8327118203104674, 'f1': 0.7600327168148598}\n\n- exp 2:\nParams: Base bert with 2FC, epoch 2, lr 1e-5\n\n{'Accuracy': 0.9931995444417273, 'precision': 0.755762387079113, 'recall': 0.7980805365247304, 'f1': 0.7763452047860748}\n\n- exp 3:\nparams: 2FC, epoch 2, lr 1e-5 with gradient clip\n{'Accuracy': 0.9932764968526464, 'precision': 0.7633003963601853, 'recall': 0.7905067499205042, 'f1': 0.7766653886025079}\n\n- exp 4: 3FC, epoch 2, 1e-5 with gradient clip\n\n{'Accuracy': 0.9933637095850213, 'precision': 0.7576469952442715, 'recall': 0.8105397045645073, 'f1': 0.7832013519364255}\n","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.plot(train_loss_data, label=\"Training loss\")\nplt.plot(valid_loss_data, label=\"validation loss\")\nplt.legend(frameon=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:51:19.579429Z","iopub.execute_input":"2022-02-14T17:51:19.579819Z","iopub.status.idle":"2022-02-14T17:51:19.830516Z","shell.execute_reply.started":"2022-02-14T17:51:19.579786Z","shell.execute_reply":"2022-02-14T17:51:19.829766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nscore_df = pd.DataFrame.from_dict(score_data_list)\nscore_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:51:22.937258Z","iopub.execute_input":"2022-02-14T17:51:22.937911Z","iopub.status.idle":"2022-02-14T17:51:22.949599Z","shell.execute_reply.started":"2022-02-14T17:51:22.937875Z","shell.execute_reply":"2022-02-14T17:51:22.948962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare For Testing","metadata":{}},{"cell_type":"markdown","source":"Load best model","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"nbme_bert_v2.pth\", map_location = DEVICE))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:51:33.339694Z","iopub.execute_input":"2022-02-14T17:51:33.339963Z","iopub.status.idle":"2022-02-14T17:51:33.565648Z","shell.execute_reply.started":"2022-02-14T17:51:33.339918Z","shell.execute_reply":"2022-02-14T17:51:33.564786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_df():\n    feats = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    test = pd.read_csv(f\"{BASE_URL}/test.csv\")\n\n    merged = test.merge(notes, how = \"left\")\n    merged = merged.merge(feats, how = \"left\")\n\n    def process_feature_text(text):\n        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n    \n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    \n    return merged\n\n\nclass SubmissionDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = self.tokenizer(\n            example[\"feature_text\"],\n            example[\"pn_history\"],\n            truncation = self.config['truncation'],\n            max_length = self.config['max_length'],\n            padding = self.config['padding'],\n            return_offsets_mapping = self.config['return_offsets_mapping']\n        )\n        tokenized[\"sequence_ids\"] = tokenized.sequence_ids()\n\n        input_ids = np.array(tokenized[\"input_ids\"])\n        attention_mask = np.array(tokenized[\"attention_mask\"])\n        token_type_ids = np.array(tokenized[\"token_type_ids\"])\n        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\")\n\n        return input_ids, attention_mask, token_type_ids, offset_mapping, sequence_ids\n\n\ntest_df = create_test_df()\n\nsubmission_data = SubmissionDataset(test_df, tokenizer, hyperparameters)\nsubmission_dataloader = DataLoader(submission_data, batch_size=hyperparameters['batch_size'], shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:14.28865Z","iopub.execute_input":"2022-02-15T04:43:14.288938Z","iopub.status.idle":"2022-02-15T04:43:14.615585Z","shell.execute_reply.started":"2022-02-15T04:43:14.288889Z","shell.execute_reply":"2022-02-15T04:43:14.614876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\npreds = []\noffsets = []\nseq_ids = []\n\nfor batch in tqdm(submission_dataloader):\n    input_ids = batch[0].to(DEVICE)\n    attention_mask = batch[1].to(DEVICE)\n    token_type_ids = batch[2].to(DEVICE)\n    offset_mapping = batch[3]\n    sequence_ids = batch[4]\n\n    logits = model(input_ids, attention_mask, token_type_ids)\n    \n    preds.append(logits.detach().cpu().numpy())\n    offsets.append(offset_mapping.numpy())\n    seq_ids.append(sequence_ids.numpy())\n\npreds = np.concatenate(preds, axis=0)\noffsets = np.concatenate(offsets, axis=0)\nseq_ids = np.concatenate(seq_ids, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:18.251294Z","iopub.execute_input":"2022-02-15T04:43:18.25185Z","iopub.status.idle":"2022-02-15T04:43:18.391331Z","shell.execute_reply.started":"2022-02-15T04:43:18.251811Z","shell.execute_reply":"2022-02-15T04:43:18.390536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_preds = get_location_predictions(preds, offsets, seq_ids, test=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:22.467394Z","iopub.execute_input":"2022-02-15T04:43:22.467991Z","iopub.status.idle":"2022-02-15T04:43:22.486548Z","shell.execute_reply.started":"2022-02-15T04:43:22.467953Z","shell.execute_reply":"2022-02-15T04:43:22.485852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(location_preds), len(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:24.683747Z","iopub.execute_input":"2022-02-15T04:43:24.684313Z","iopub.status.idle":"2022-02-15T04:43:24.690863Z","shell.execute_reply.started":"2022-02-15T04:43:24.684272Z","shell.execute_reply":"2022-02-15T04:43:24.689946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"location\"] = location_preds","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:26.979602Z","iopub.execute_input":"2022-02-15T04:43:26.980157Z","iopub.status.idle":"2022-02-15T04:43:26.98502Z","shell.execute_reply.started":"2022-02-15T04:43:26.980118Z","shell.execute_reply":"2022-02-15T04:43:26.984202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[[\"id\", \"location\"]].to_csv(\"submission.csv\", index = False)\npd.read_csv(\"submission.csv\").head()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:29.07465Z","iopub.execute_input":"2022-02-15T04:43:29.074938Z","iopub.status.idle":"2022-02-15T04:43:29.093071Z","shell.execute_reply.started":"2022-02-15T04:43:29.07489Z","shell.execute_reply":"2022-02-15T04:43:29.092348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Special Credits\n- [tomohiroh](https://www.kaggle.com/tomohiroh/nbme-bert-for-beginners)\n- [gazu468](https://www.kaggle.com/gazu468/nbme-details-eda)\n","metadata":{}}]}