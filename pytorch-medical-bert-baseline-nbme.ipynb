{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NBME - Score Clinical Patient Notes \n- **Framework:** Pytorch\n- **Model Architecture:**\n    - BERT\n    - Linear(768, 512)\n    - Linear(512, 1)\n- **LR:** 1e-5\n- **Batch Size:** 8\n- **Epoch:** 6\n- **Dropout:** 0.2\n- **Criterion:** BCEWithLogitsLoss\n- **Optimizer:** AdamW\n\n# Tokenizer params\n- **Max Lenght:** 416\n- **Padding:** max_lenght\n- **Truncation:** only_scond\n","metadata":{}},{"cell_type":"code","source":"from ast import literal_eval\nfrom itertools import chain\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:51:31.958937Z","iopub.execute_input":"2022-03-18T14:51:31.959172Z","iopub.status.idle":"2022-03-18T14:51:39.678044Z","shell.execute_reply.started":"2022-03-18T14:51:31.959099Z","shell.execute_reply":"2022-03-18T14:51:39.676985Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions\n### 1. Datasets Helper Function\nneed to merge `features.csv`, `patient_notes.csv` with `train.csv`","metadata":{}},{"cell_type":"code","source":"BASE_URL = \"../input/nbme-score-clinical-patient-notes\"\n\n\ndef process_feature_text(text):\n    return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n\n\ndef prepare_datasets():\n    features = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    df = pd.read_csv(f\"{BASE_URL}/train.csv\")\n    df[\"annotation_list\"] = [literal_eval(x) for x in df[\"annotation\"]]\n    df[\"location_list\"] = [literal_eval(x) for x in df[\"location\"]]\n\n    merged = df.merge(notes, how=\"left\")\n    merged = merged.merge(features, how=\"left\")\n\n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n\n    return merged","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:51:39.682012Z","iopub.execute_input":"2022-03-18T14:51:39.682307Z","iopub.status.idle":"2022-03-18T14:51:39.694470Z","shell.execute_reply.started":"2022-03-18T14:51:39.682277Z","shell.execute_reply":"2022-03-18T14:51:39.693775Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### 2. Tokenizer Helper Function","metadata":{}},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\n\ndef tokenize_and_add_labels(tokenizer, data, config):\n    out = tokenizer(\n        data[\"feature_text\"],\n        data[\"pn_history\"],\n        truncation=config['truncation'],\n        max_length=config['max_length'],\n        padding=config['padding'],\n        return_offsets_mapping=config['return_offsets_mapping']\n    )\n    labels = [0.0] * len(out[\"input_ids\"])\n    out[\"location_int\"] = loc_list_to_ints(data[\"location_list\"])\n    out[\"sequence_ids\"] = out.sequence_ids()\n\n    for idx, (seq_id, offsets) in enumerate(zip(out[\"sequence_ids\"], out[\"offset_mapping\"])):\n        if not seq_id or seq_id == 0:\n            labels[idx] = -1\n            continue\n\n        token_start, token_end = offsets\n        for feature_start, feature_end in out[\"location_int\"]:\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                break\n\n    out[\"labels\"] = labels\n\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:51:39.696161Z","iopub.execute_input":"2022-03-18T14:51:39.696661Z","iopub.status.idle":"2022-03-18T14:51:39.710349Z","shell.execute_reply.started":"2022-03-18T14:51:39.696627Z","shell.execute_reply":"2022-03-18T14:51:39.709520Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### 3. Prediction and Score Helper Function","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef get_location_predictions(preds, offset_mapping, sequence_ids, test=False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        pred = 1 / (1 + np.exp(-pred))\n        start_idx = None\n        end_idx = None\n        current_preds = []\n        for pred, offset, seq_id in zip(pred, offsets, seq_ids):\n            if seq_id is None or seq_id == 0:\n                continue\n\n            if pred > 0.5:\n                if start_idx is None:\n                    start_idx = offset[0]\n                end_idx = offset[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n            \n    return all_predictions\n\n\ndef calculate_char_cv(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros(num_chars)\n\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n\n        char_preds = np.zeros(num_chars)\n\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n\n    results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", labels=np.unique(all_preds))\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n\n    return {\n        \"Accuracy\": accuracy,\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:51:39.712781Z","iopub.execute_input":"2022-03-18T14:51:39.713092Z","iopub.status.idle":"2022-03-18T14:51:39.728407Z","shell.execute_reply.started":"2022-03-18T14:51:39.713056Z","shell.execute_reply":"2022-03-18T14:51:39.727347Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data = self.data.iloc[idx]\n        tokens = tokenize_and_add_labels(self.tokenizer, data, self.config)\n\n        input_ids = np.array(tokens[\"input_ids\"])\n        attention_mask = np.array(tokens[\"attention_mask\"])\n        token_type_ids = np.array(tokens[\"token_type_ids\"])\n\n        labels = np.array(tokens[\"labels\"])\n        offset_mapping = np.array(tokens['offset_mapping'])\n        sequence_ids = np.array(tokens['sequence_ids']).astype(\"float16\")\n        \n        return input_ids, attention_mask, token_type_ids, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:51:39.730561Z","iopub.execute_input":"2022-03-18T14:51:39.731426Z","iopub.status.idle":"2022-03-18T14:51:39.740686Z","shell.execute_reply.started":"2022-03-18T14:51:39.731385Z","shell.execute_reply":"2022-03-18T14:51:39.740038Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- Lets use **BERT** base Architecture\n- Also Used 3 FC layers\n\n**Comments:** 3 layers improve accuracy 2% on public score","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass CustomModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(config['model_name'])  # BERT model\n        self.dropout = nn.Dropout(p=config['dropout'])\n        self.config = config\n        self.fc1 = nn.Linear(768, 512)\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512, 1)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        logits = self.fc1(outputs[0])\n        logits = self.fc2(self.dropout(logits))\n        logits = self.fc3(self.dropout(logits)).squeeze(-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:51:39.742010Z","iopub.execute_input":"2022-03-18T14:51:39.742394Z","iopub.status.idle":"2022-03-18T14:51:39.753436Z","shell.execute_reply.started":"2022-03-18T14:51:39.742359Z","shell.execute_reply":"2022-03-18T14:51:39.752750Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters\n","metadata":{}},{"cell_type":"code","source":"hyperparameters = {\n    \"max_length\": 416,\n    \"padding\": \"max_length\",\n    \"return_offsets_mapping\": True,\n    \"truncation\": \"only_second\",\n    \"model_name\": \"../input/huggingface-bert/bert-base-uncased\",\n    \"dropout\": 0.2,\n    \"lr\": 1e-5,\n    \"test_size\": 0.2,\n    \"seed\": 1268,\n    \"batch_size\": 8\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:51:39.754923Z","iopub.execute_input":"2022-03-18T14:51:39.755929Z","iopub.status.idle":"2022-03-18T14:51:39.762290Z","shell.execute_reply.started":"2022-03-18T14:51:39.755848Z","shell.execute_reply":"2022-03-18T14:51:39.761522Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Datasets\nTrain and Test split: 20%\n\nTotal Data:\n- Train: 11440\n- Test: 2860","metadata":{}},{"cell_type":"code","source":"train_df = prepare_datasets()\n\nX_train, X_test = train_test_split(train_df, test_size=hyperparameters['test_size'],\n                                   random_state=hyperparameters['seed'])\n\n\nprint(\"Train size\", len(X_train))\nprint(\"Test Size\", len(X_test))","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:51:51.226218Z","iopub.execute_input":"2022-03-18T14:51:51.226466Z","iopub.status.idle":"2022-03-18T14:51:51.808788Z","shell.execute_reply.started":"2022-03-18T14:51:51.226439Z","shell.execute_reply":"2022-03-18T14:51:51.808013Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Train size 11440\nTest Size 2860\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(hyperparameters['model_name'])\n\ntraining_data = CustomDataset(X_train, tokenizer, hyperparameters)\ntrain_dataloader = DataLoader(training_data, batch_size=hyperparameters['batch_size'], shuffle=True)\n\ntest_data = CustomDataset(X_test, tokenizer, hyperparameters)\ntest_dataloader = DataLoader(test_data, batch_size=hyperparameters['batch_size'], shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:51:51.810387Z","iopub.execute_input":"2022-03-18T14:51:51.810814Z","iopub.status.idle":"2022-03-18T14:51:51.882749Z","shell.execute_reply.started":"2022-03-18T14:51:51.810775Z","shell.execute_reply":"2022-03-18T14:51:51.882074Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Train\nLets train the model\nwith BCEWithLogitsLoss and AdamW as optimizer\n\n**Notes:** on BCEWithLogitsLoss, the default value for reduction is `mean` (the sum of the output will be divided by the number of elements in the output). If we use this default value, it will produce negative loss. Because we have some negative labels. To fix this negative loss issue, we can use `none` as parameter. To calculate the mean, first, we have to filter out the negative values. [DOC](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = CustomModel(hyperparameters).to(DEVICE)\n\ncriterion = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\noptimizer = optim.AdamW(model.parameters(), lr=hyperparameters['lr'])","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:51:56.185573Z","iopub.execute_input":"2022-03-18T14:51:56.186123Z","iopub.status.idle":"2022-03-18T14:52:06.060695Z","shell.execute_reply.started":"2022-03-18T14:51:56.186085Z","shell.execute_reply":"2022-03-18T14:52:06.059935Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-bert/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, criterion):\n        model.train()\n        train_loss = []\n\n        for batch in tqdm(dataloader):\n            optimizer.zero_grad()\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            token_type_ids = batch[2].to(DEVICE)\n            labels = batch[3].to(DEVICE)\n\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = criterion(logits, labels)\n            # since, we have\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            train_loss.append(loss.item() * input_ids.size(0))\n            loss.backward()\n            # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n            # it's also improve f1 accuracy slightly\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n        return sum(train_loss)/len(train_loss)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:52:06.063057Z","iopub.execute_input":"2022-03-18T14:52:06.063557Z","iopub.status.idle":"2022-03-18T14:52:06.071992Z","shell.execute_reply.started":"2022-03-18T14:52:06.063519Z","shell.execute_reply":"2022-03-18T14:52:06.071235Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, dataloader, criterion):\n        model.eval()\n        valid_loss = []\n        preds = []\n        offsets = []\n        seq_ids = []\n        valid_labels = []\n\n        for batch in tqdm(dataloader):\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            token_type_ids = batch[2].to(DEVICE)\n            labels = batch[3].to(DEVICE)\n            offset_mapping = batch[4]\n            sequence_ids = batch[5]\n\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = criterion(logits, labels)\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            valid_loss.append(loss.item() * input_ids.size(0))\n\n            preds.append(logits.detach().cpu().numpy())\n            offsets.append(offset_mapping.numpy())\n            seq_ids.append(sequence_ids.numpy())\n            valid_labels.append(labels.detach().cpu().numpy())\n\n        preds = np.concatenate(preds, axis=0)\n        offsets = np.concatenate(offsets, axis=0)\n        seq_ids = np.concatenate(seq_ids, axis=0)\n        valid_labels = np.concatenate(valid_labels, axis=0)\n        location_preds = get_location_predictions(preds, offsets, seq_ids, test=False)\n        score = calculate_char_cv(location_preds, offsets, seq_ids, valid_labels)\n\n        return sum(valid_loss)/len(valid_loss), score","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:52:06.073403Z","iopub.execute_input":"2022-03-18T14:52:06.073646Z","iopub.status.idle":"2022-03-18T14:52:06.099287Z","shell.execute_reply.started":"2022-03-18T14:52:06.073615Z","shell.execute_reply":"2022-03-18T14:52:06.098598Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import time\n\ntrain_loss_data, valid_loss_data = [], []\nscore_data_list = []\nvalid_loss_min = np.Inf\nsince = time.time()\nepochs = 6","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:52:06.101969Z","iopub.execute_input":"2022-03-18T14:52:06.102449Z","iopub.status.idle":"2022-03-18T14:52:06.110483Z","shell.execute_reply.started":"2022-03-18T14:52:06.102411Z","shell.execute_reply":"2022-03-18T14:52:06.109768Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"best_loss = np.inf\n\nfor i in range(epochs):\n    print(\"Epoch: {}/{}\".format(i + 1, epochs))\n    # first train model\n    train_loss = train_model(model, train_dataloader, optimizer, criterion)\n    train_loss_data.append(train_loss)\n    print(f\"Train loss: {train_loss}\")\n    # evaluate model\n    valid_loss, score = eval_model(model, test_dataloader, criterion)\n    valid_loss_data.append(valid_loss)\n    score_data_list.append(score)\n    print(f\"Valid loss: {valid_loss}\")\n    print(f\"Valid score: {score}\")\n    \n    if valid_loss < best_loss:\n        best_loss = valid_loss\n        torch.save(model.state_dict(), \"nbme_bert_v2.pth\")\n\n    \ntime_elapsed = time.time() - since\nprint('Training completed in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))","metadata":{"execution":{"iopub.status.busy":"2022-03-18T14:53:26.958312Z","iopub.execute_input":"2022-03-18T14:53:26.958554Z","iopub.status.idle":"2022-03-18T15:56:29.294650Z","shell.execute_reply.started":"2022-03-18T14:53:26.958529Z","shell.execute_reply":"2022-03-18T15:56:29.293945Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch: 1/6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a41508d851546f084734eb051c5a659"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.32089299614888417\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dbf6369c8ee4e5f930f5437c7ad39ca"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.16514019701882054\nValid score: {'Accuracy': 0.9913787648967042, 'precision': 0.6771357709402549, 'recall': 0.7971265863035875, 'f1': 0.7322481278878326}\nEpoch: 2/6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d98ed26613b456b8ac70db456694d8e"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.13714047926452624\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bbb9149183b4f729f331988b5da247b"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.13208059790403928\nValid score: {'Accuracy': 0.992829317842528, 'precision': 0.7360890302066773, 'recall': 0.8030526407076576, 'f1': 0.768114138774833}\nEpoch: 3/6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"108b9be62fb0442aa23ddc7d3287e793"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.09756921141446893\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"770d5611f8ec4da1ad015e45c392f462"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.12147204848258014\nValid score: {'Accuracy': 0.9934808482549758, 'precision': 0.7630977639954301, 'recall': 0.8109444107189315, 'f1': 0.7862938826991802}\nEpoch: 4/6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeb56c5b85c04b1ba9f922375f190d55"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.07530133352077796\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cff5b349b60c46609cf8ac085f350fce"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.11958435300060803\nValid score: {'Accuracy': 0.9936710917153034, 'precision': 0.7594053954854102, 'recall': 0.8373370335038881, 'f1': 0.7964694236691597}\nEpoch: 5/6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8f74682890a46438b3a5606363940fb"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.057191896206607314\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bc9040083d047c1a6689130e4fa168b"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.12776436868144453\nValid score: {'Accuracy': 0.9936227827017821, 'precision': 0.7528528837257119, 'recall': 0.8467609053854827, 'f1': 0.7970503802669351}\nEpoch: 6/6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cedd854ecf04668b3730a5672de3fe0"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.04701529008692782\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc301540848f4d599e5a3d11e45cd964"}},"metadata":{}},{"name":"stdout","text":"Valid loss: 0.1342339918991906\nValid score: {'Accuracy': 0.9935744736882607, 'precision': 0.7513555184376205, 'recall': 0.845228803515162, 'f1': 0.795532459052076}\nTraining completed in 64m 23s\n","output_type":"stream"}]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.plot(train_loss_data, label=\"Training loss\")\nplt.plot(valid_loss_data, label=\"validation loss\")\nplt.legend(frameon=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T12:20:51.679384Z","iopub.execute_input":"2022-03-18T12:20:51.679957Z","iopub.status.idle":"2022-03-18T12:20:51.765019Z","shell.execute_reply.started":"2022-03-18T12:20:51.679882Z","shell.execute_reply":"2022-03-18T12:20:51.763901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nscore_df = pd.DataFrame.from_dict(score_data_list)\nscore_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:51:22.937258Z","iopub.execute_input":"2022-02-14T17:51:22.937911Z","iopub.status.idle":"2022-02-14T17:51:22.949599Z","shell.execute_reply.started":"2022-02-14T17:51:22.937875Z","shell.execute_reply":"2022-02-14T17:51:22.948962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare For Testing","metadata":{}},{"cell_type":"markdown","source":"Load best model","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"nbme_bert_v2.pth\", map_location = DEVICE))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T17:51:33.339694Z","iopub.execute_input":"2022-02-14T17:51:33.339963Z","iopub.status.idle":"2022-02-14T17:51:33.565648Z","shell.execute_reply.started":"2022-02-14T17:51:33.339918Z","shell.execute_reply":"2022-02-14T17:51:33.564786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_df():\n    feats = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    test = pd.read_csv(f\"{BASE_URL}/test.csv\")\n\n    merged = test.merge(notes, how = \"left\")\n    merged = merged.merge(feats, how = \"left\")\n\n    def process_feature_text(text):\n        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n    \n    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n    \n    return merged\n\n\nclass SubmissionDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = self.tokenizer(\n            example[\"feature_text\"],\n            example[\"pn_history\"],\n            truncation = self.config['truncation'],\n            max_length = self.config['max_length'],\n            padding = self.config['padding'],\n            return_offsets_mapping = self.config['return_offsets_mapping']\n        )\n        tokenized[\"sequence_ids\"] = tokenized.sequence_ids()\n\n        input_ids = np.array(tokenized[\"input_ids\"])\n        attention_mask = np.array(tokenized[\"attention_mask\"])\n        token_type_ids = np.array(tokenized[\"token_type_ids\"])\n        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\")\n\n        return input_ids, attention_mask, token_type_ids, offset_mapping, sequence_ids\n\n\ntest_df = create_test_df()\n\nsubmission_data = SubmissionDataset(test_df, tokenizer, hyperparameters)\nsubmission_dataloader = DataLoader(submission_data, batch_size=hyperparameters['batch_size'], shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:14.28865Z","iopub.execute_input":"2022-02-15T04:43:14.288938Z","iopub.status.idle":"2022-02-15T04:43:14.615585Z","shell.execute_reply.started":"2022-02-15T04:43:14.288889Z","shell.execute_reply":"2022-02-15T04:43:14.614876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\npreds = []\noffsets = []\nseq_ids = []\n\nfor batch in tqdm(submission_dataloader):\n    input_ids = batch[0].to(DEVICE)\n    attention_mask = batch[1].to(DEVICE)\n    token_type_ids = batch[2].to(DEVICE)\n    offset_mapping = batch[3]\n    sequence_ids = batch[4]\n\n    logits = model(input_ids, attention_mask, token_type_ids)\n    \n    preds.append(logits.detach().cpu().numpy())\n    offsets.append(offset_mapping.numpy())\n    seq_ids.append(sequence_ids.numpy())\n\npreds = np.concatenate(preds, axis=0)\noffsets = np.concatenate(offsets, axis=0)\nseq_ids = np.concatenate(seq_ids, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:18.251294Z","iopub.execute_input":"2022-02-15T04:43:18.25185Z","iopub.status.idle":"2022-02-15T04:43:18.391331Z","shell.execute_reply.started":"2022-02-15T04:43:18.251811Z","shell.execute_reply":"2022-02-15T04:43:18.390536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_preds = get_location_predictions(preds, offsets, seq_ids, test=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:22.467394Z","iopub.execute_input":"2022-02-15T04:43:22.467991Z","iopub.status.idle":"2022-02-15T04:43:22.486548Z","shell.execute_reply.started":"2022-02-15T04:43:22.467953Z","shell.execute_reply":"2022-02-15T04:43:22.485852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(location_preds), len(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:24.683747Z","iopub.execute_input":"2022-02-15T04:43:24.684313Z","iopub.status.idle":"2022-02-15T04:43:24.690863Z","shell.execute_reply.started":"2022-02-15T04:43:24.684272Z","shell.execute_reply":"2022-02-15T04:43:24.689946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"location\"] = location_preds","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:26.979602Z","iopub.execute_input":"2022-02-15T04:43:26.980157Z","iopub.status.idle":"2022-02-15T04:43:26.98502Z","shell.execute_reply.started":"2022-02-15T04:43:26.980118Z","shell.execute_reply":"2022-02-15T04:43:26.984202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[[\"id\", \"location\"]].to_csv(\"submission.csv\", index = False)\npd.read_csv(\"submission.csv\").head()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:43:29.07465Z","iopub.execute_input":"2022-02-15T04:43:29.074938Z","iopub.status.idle":"2022-02-15T04:43:29.093071Z","shell.execute_reply.started":"2022-02-15T04:43:29.07489Z","shell.execute_reply":"2022-02-15T04:43:29.092348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Special Credits\n- [tomohiroh](https://www.kaggle.com/tomohiroh/nbme-bert-for-beginners)\n- [gazu468](https://www.kaggle.com/gazu468/nbme-details-eda)\n","metadata":{}}]}